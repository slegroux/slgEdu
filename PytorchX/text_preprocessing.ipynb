{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython import embed\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "import torchtext.datasets\n",
    "from torchtext.datasets import language_modeling, LanguageModelingDataset, UDPOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simply remove non alphabetical characters and lowercase everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd']"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [['a', 'b'],['c','d']]\n",
    "[ c for item in t for c in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader(object):\n",
    "    def __init__(self, fn:str, mode:str='word'):\n",
    "        self.mode = mode\n",
    "        self.path = Path(fn)\n",
    "        self.data = self.read_data(self.path)\n",
    "\n",
    "    def read_data(self, path:Path):\n",
    "        data = []\n",
    "        with open(path, mode='r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            data.append(self.tokenizer(line, self.mode))\n",
    "        if self.mode == 'char':\n",
    "            return [ c for row in data for c in row ]\n",
    "        else:\n",
    "            return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(sentence:str, mode:str='word'):\n",
    "        \n",
    "        if mode == 'word':\n",
    "            l = re.sub('[^A-Za-z]+',' ', sentence.strip().lower()).split()\n",
    "        elif mode == 'char':\n",
    "            l = list(re.sub('[^A-Za-z]+',' ', sentence.strip().lower()))\n",
    "        else:\n",
    "            logging.error('unknown token type mode', mode)\n",
    "        return(l)\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        return(self.data[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataReader('data/35.txt', mode='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 's', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(d[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'i', 's', ' ', 'i', 's', ' ', 't', 'r', 'u', 'e', ' ', 'a', 't', ' ', 'l', 'a', 'e', 's', 't', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(d.tokenizer('this is true!\\n 34 at laest!', mode='char'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p 189604\n"
     ]
    }
   ],
   "source": [
    "print(d[0], len(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'p': 2870,\n",
       "         'r': 8835,\n",
       "         'o': 11083,\n",
       "         'j': 185,\n",
       "         'e': 19670,\n",
       "         'c': 4027,\n",
       "         't': 15042,\n",
       "         ' ': 33709,\n",
       "         'g': 3491,\n",
       "         'u': 4310,\n",
       "         'n': 10945,\n",
       "         'b': 2172,\n",
       "         's': 9244,\n",
       "         'h': 8787,\n",
       "         'i': 11257,\n",
       "         'm': 4411,\n",
       "         'a': 12703,\n",
       "         'y': 3001,\n",
       "         'w': 3496,\n",
       "         'l': 6629,\n",
       "         'k': 1219,\n",
       "         'f': 3735,\n",
       "         'd': 6860,\n",
       "         'v': 1407,\n",
       "         'x': 264,\n",
       "         'z': 146,\n",
       "         'q': 106})"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 33709),\n",
       " ('e', 19670),\n",
       " ('t', 15042),\n",
       " ('a', 12703),\n",
       " ('i', 11257),\n",
       " ('o', 11083),\n",
       " ('n', 10945),\n",
       " ('s', 9244),\n",
       " ('r', 8835),\n",
       " ('h', 8787)]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(Object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def numericalize(corpus:list):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing using Torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no it was n't black monday \n",
      " but while the new york stock exchange did n't fall apart friday as the dow jones industrial average plunged N points most of it in the final hour it barely managed to stay this side of chaos \n",
      " some circuit breakers installed after the october N crash failed their first test traders say unable to cool the selling panic in both stocks and futures \n",
      " the N stock specialist firms on the big board floor the buyers and sellers of last resort who were criticized after the N crash once again could n't handle the selling pressure \n",
      " big investment banks refused to step up to the plate to support the beleaguered floor traders by buying big blocks of stock traders say \n",
      " heavy selling of standard & poor 's 500-stock index futures in chicago <unk> beat stocks downward \n",
      " seven big board stocks ual amr bankamerica walt disney capital cities\\/abc philip morris and pacific telesis group stopped trading and never resumed \n",
      " the <unk> has already begun \n",
      " the equity market was <unk> \n",
      " once again the specialists were not able to handle the imbalances on the floor of the new york stock exchange said christopher <unk> senior vice president at <unk> securities corp \n"
     ]
    }
   ],
   "source": [
    "!head data/ptb.test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok(sentence:str):\n",
    "    return(d.tokenizer(sentence, mode='word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, \\\n",
    "                  sequential=True, \\\n",
    "                  use_vocab=True, \\\n",
    "                  eos_token=\"<eos>\", \\\n",
    "                  init_token=\"<bos>\", \\\n",
    "                  tokenize=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('text', TEXT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toto', 'is', 'here']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"toto is here! 42\"\n",
    "print(TEXT.preprocess(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = data.TabularDataset(path='data/ptb.test.txt',format='csv',skip_header=False, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'it', 'was', 'n', 't', 'black', 'monday']\n"
     ]
    }
   ],
   "source": [
    "print(ds.examples[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('unk', 4794), ('the', 4541), ('n', 2873), ('of', 2195), ('to', 2043), ('a', 1855), ('in', 1640), ('and', 1540), ('s', 1162), ('that', 831)]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(ds)\n",
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "unk\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.stoi['unk'])\n",
    "print(TEXT.vocab.itos[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mLanguageModelingDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_field\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnewline_eos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Defines a dataset for language modeling.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a LanguageModelingDataset given a path and a field.\n",
       "\n",
       "Arguments:\n",
       "    path: Path to the data file.\n",
       "    text_field: The field that will be used for text data.\n",
       "    newline_eos: Whether to add an <eos> token for every newline in the\n",
       "        data file. Default: True.\n",
       "    Remaining keyword arguments: Passed to the constructor of\n",
       "        data.Dataset.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/rnnlm/lib/python3.7/site-packages/torchtext/datasets/language_modeling.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     WikiText2, WikiText103, PennTreebank\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?LanguageModelingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModelingDataset('data/ptb.test.txt', TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-418-d585e7ebf0f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "train, test = lm.splits(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Create Dataset objects for multiple splits of a dataset.\n",
       "\n",
       "Arguments:\n",
       "    path (str): Common prefix of the splits' file paths, or None to use\n",
       "        the result of cls.download(root).\n",
       "    root (str): Root dataset storage directory. Default is '.data'.\n",
       "    train (str): Suffix to add to path for the train set, or None for no\n",
       "        train set. Default is None.\n",
       "    validation (str): Suffix to add to path for the validation set, or None\n",
       "        for no validation set. Default is None.\n",
       "    test (str): Suffix to add to path for the test set, or None for no test\n",
       "        set. Default is None.\n",
       "    Remaining keyword arguments: Passed to the constructor of the\n",
       "        Dataset (sub)class being used.\n",
       "\n",
       "Returns:\n",
       "    Tuple[Dataset]: Datasets for train, validation, and\n",
       "    test splits in that order, if provided.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/rnnlm/lib/python3.7/site-packages/torchtext/data/dataset.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?lm.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
